\chapter{The Monophoton Analysis}
\label{chap:analysis}

In this chapter, we discuss the search for dark matter produced in association with a single high-\pt photon.
Our benchmark signal models are the vector and axial dark matter mediators discussed previously. 
However, many Standard Model (SM) processes are also capable of producing events with a single high-\pt\ photon and large \met, whether with real photons, other particles misidentified as photons, or unphysical photon signatures from various  machine and detector processes. \PH{However, many doesn't read well. I think this first paragraph deserves an actual introduction to the analysis and not an immediate dive into the details}
\PH{You immediately start talkinga bout the backgrounds. Would be better to put some diagrams and discussion about the final state. This you can tie in with the above comment.}

%%% maybe thrown in comparison diagrams here

The most significant are the irreducible backgrounds, where the underlying physics process produces the exact same signature as the signal with only real and properly identified physics objects.
In this case, the irreducible backgrounds are the associated production of a high-energy \Pgg\ with either a \PZ\ boson that subsequently decays to a pair of neutrinos or a \PW\ boson that decays to a charged lepton and a neutrino, with the charged lepton outside of the detector acceptance.
These two processes are denoted as \zinvg\ and \wlng, respectively, and together they account for approximately 70\% of the SM background, with 50\% from the former and 20\% from the latter.
The modeling of the irreducible backgrounds is explained in Section~\ref{sec:irreducible}.
% The total irreducible background rate is estimated using simultaneous fits to the signal and control regions defined in Section~\ref{sec:event_selection} and 

Additional backgrounds arise from events where the candidate photon object is a misidentified electron (Section~\ref{sec:efake}) or an electromagnetic shower caused by hadrons (Section~\ref{sec:hfake}). 
The background events from electron misidentification are mostly \PW\ boson production ($\PW\rightarrow \Pe\Pgn$), whereas those from hadron misidentification are due to multiple sources such as $\PZ(\rightarrow\Pgn\Pgn)+\text{jets}$ and QCD multijets with grossly mismeasured jet energy. 
Misidentification itself is rare, but because these processes have high cross sections, the amount of background is substantial, approximately 15\% and 5\% respectively.
Since object misidentification rates depend on subtle details of the detector, the MC simulation often fails to accurately describe them.
Therefore, the contributions from these background processes are estimated by employing data-driven techniques, where the misidentification rates are measured in data and applied to proxy samples with well-identified electrons or hadrons.

Finally, apparent large energy deposits in ECAL from non-collision processes mimic \gmet\ events and therefore need to be controlled. 
Known sources of such background include bremsstrahlung of beam halo or cosmic ray muons and anomalous ECAL energy deposits resulting from the interaction of particles in the ECAL photodetectors referred to as ``ECAL spikes''. 
These methods used to estimate the contributions from these processes are described in detail in Sections~\ref{sec:halo_estimate} and~\ref{sec:spike_estimate}, respectively.

The estimates of the contributions from \zinvg, \wlng, and beam halo processes are allowed to float in the fits to data performed to extract the potential signal contribution and set limits on new physics models. %  described in Section~\ref{sec:interpretation}.
Meanwhile, for all other background processes, the yields in the fits are fixed to the estimates from data-driven methods or MC cross section calculation. 
\PH{The above paragraph doesn't fit. You bring up fit out of nowhere and floating is jargon. I would just remove this and outline this when you do the signal extraction}

\section{Dataset}
\label{sec:dataset}

The data sample was collected with a single-photon trigger: \texttt{HLT\_Photon165\_HE10}.
This high-level trigger algorithm is relatively simple, only requiring at least one photon candidate with $\ET > 165\GeV$ reconstructed with a corresponding L1 seed.
The photon candidate must also have $H/E < 0.1$ to discriminate against jets, where $H/E$ is the ratio of HCAL to ECAL energy deposits in the central calorimeter tower corresponding to the candidate.
The photon energy reconstructed at the HLT is less precise relative to that derived later in the offline reconstruction. 
Therefore, the online thresholds in the trigger on both $H/E$ and \ETg are less restrictive than their offline counterparts.

The trigger efficiency is measured to be about 98\% for events passing the analysis selection with $\ETg > 175\GeV$ and the integrated luminosity of the analyzed data sample is $(35.9\pm0.9)$\fbinv~\cite{CMS:2017sdi}.

\subsection{Trigger Efficiency}
\label{sec:triggereff}

\input{Analysis/trigger}

\subsection{Pileup Reweighting}
\label{sec:puweight}

The distribution of the number of pileup interactions inserted into MC events differ from the true pileup distribution, estimated from the measurement of instantaneous luminosity, beam intensity of each proton bunch, and the total cross section of proton inelastic scattering (69.2 $\textrm{mb}^{-1}$). \PH{we should know htis calculation each proton bunch multiplied by the total cross...}

Figure~\ref{fig:pudist} shows the pileup distributions in data and MC and the ratio of the two. 
Each simulated MC event has its weight multiplied by the value of the ratio evaluated at the number of true pileup interaction injected into the event in order to reproduce the observed pileup distribution in the MC samples. 

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.48\textwidth]{Analysis/Figures/PUMoriond17.pdf}
  \caption{
    The pileup distributions in data and MC.
    The ratio of the two is used to reweight the MC samples.
  }
  \label{fig:pudist}
\end{figure}

\section{Event Selection}
\label{sec:event_selection}

From the recorded data, events are selected by requiring $\met > 170\GeV$ and at least one photon with $\ETg > 175\GeV$ in the fiducial region of the ECAL barrel ($\abs{\eta} < 1.44$).
Events with photons in the endcaps are not considered because the estimation of backgrounds due to beam halo and misidentified hadron is greatly complicated because the crystals in the ECAL endcaps do not form a grid in $\eta$ and $\phi$. 

Events with a high-\pt\ photon and large \met\ are subjected to further requirements to suppress SM background processes that feature a genuine high-energy photon, but not a significant amount of \met.
One such SM process is \gj, where an apparently large \met\ is often the result of mismeasuring the energy of a jet.
In contrast to signal-like processes, the \met is typically smaller than \ETg\ in these events, so requiring the ratio of \ETg\ to \met to be less than 1.4 rejects this background effectively with little effect on signal efficiency. 
Events are also rejected if the minimum opening angle between \ptvecmiss\ and the directions of the four highest \pt\ jets, \mindphijmet, is less than 0.5; only jets with $\pt > 30\GeV$ and $\abs{\eta} < 5$ are considered in the \mindphijmet\ calculation. 
In the \gj\ process, rare pathological mismeasurements of \ETg\ also lead to large \met. 
For this reason, the candidate photon \ptvec\ and \ptvecmiss\ must be separated by more than 0.5 radians.
Figure~\ref{fig:n-1} shows the N-1 plots for various selections in the signal region.

\begin{figure}[htbp]
  \centering
  \resizebox{\textwidth}{!}{
    \includegraphics[]{Analysis/Figures/selection/recoilScan.pdf}
    \includegraphics[]{Analysis/Figures/selection/phoPtOverMet.pdf}
  }
  \resizebox{\textwidth}{!}{
    \includegraphics[]{Analysis/Figures/selection/dPhiJetMetMin.pdf}
    \includegraphics[]{Analysis/Figures/selection/dPhiPhoMet.pdf}
  }
  \caption{
    Signal region N-1 plots for the \met\ (top-left), $\ETg / \met$ (top-right), \mindphijmet\ (bottom-left), and \dphigmet\ (bottom-right) selections.
    The final selections are chosen to reduce the rate of reducible SM backgrounds, particularly from the \gj\ process.
  }
  \label{fig:n-1}
\end{figure}

\begin{table}[htbp]
  \centering
  \resizebox{\textwidth}{!}{
    \begin{tabular}{l | l | r}
      Variable & Selection & Motivation \\
      \hline
      $\ETg$ & $ > 175\GeV$ & high-\pt\ photon passing trigger \\
      $\abs{\eta}$ & $ < 1.44$ & region with best background estimates \\
      $\met $ & $ > 170\GeV$ & characteristic signature of dark matter \\
      $\ETg / \met  $ & $ < 1.4$ & reduce jet mismeasurement backgrounds \\
      $\mindphijmet  $ & $ < 0.5$ & reduce jet mismeasurement backgrounds \\
      $\dphigmet  $ & $ > 0.5$ & reduce photon mismeasurement backgrounds \\
    \end{tabular}
    }
  \caption{Baseline selections for all events considered in the analysis.}
  \label{tab:baseline}
\end{table}

The above selections, summarized in Table~\ref{tab:baseline}, constitute the baseline selections common to all regions.
To improve the purity of the signal region, we require a more stringent photon identification as well as additional object vetos.
The contributions from the \zinvg\ and \wlng\ processes to the signal region are modeled by fitting to observed data in control regions where one or two leptons (electrons or muons) are identified in addition to the photon candidate.
Meanwhile, the contributions from misidentified electrons and hadrons are modeled by proxy regions where some of the selections in the photon identification have been inverted. 
The additional requirements for the signal and various control regions used in the analysis are described in the following sections. 

\subsection{Signal Regions}
\label{sec:signal_regions}

The defining feature of the signal region is the application of both the $\egamma$ and \Pgg-specific portions of the photon ID, given in Tables~\ref{tab:egid} and~\ref{tab:gsid} respectively.
The former reduces the hadron misidentification rate with a collection of isolation and shower shape selections while the latter reduces the electron misidentification rate with the pixel seed veto and rejects non-collisions backgrounds with specifically tailored selections.

In the signal region, events are vetoed if they contain an electron or a muon with $\pt > 10\GeV$ that is separated from the photon by $\dR > 0.5$. This lepton veto rejects SM processes that produce a high-\pt\ photon, \met, and leptons such as \wlng, \ttg, and $VV\Pgg$. %% add a plot ???

Furthermore, to constrain the beam halo normalization, the signal region is split into two parts according to the variable $\phi'$ introduced in Equation~\ref{eqn:phi}. 
The region defined by $\abs{\phi'} < 0.5$ is called the horizontal region, its complement $0.5 < \abs{\phi'} < \pi/2$ is called the vertical region, and the two together are referred to as the combined signal regions.
\PH{I think its fine to discuss the splitting of the signal region into categories later. Here, it just makes it hard to follow}

\subsection{Control Regions}
\label{sec:control_regions}
\PH{Can you make a table of control regions and what they are used for and variables you are fitting, you can add the signal too. A diagram would also suffice. }

The single-electron (single-muon) control region is defined by requiring exactly one tight electron (muon) with $\pt > 30\GeV$ and $\abs{\eta} < 2.5\,(2.4)$ in addition to requiring the same photon ID as in the signal regions.
To suppress the contributions from large-\met\ processes other than \wlng, the transverse mass $\mt = \sqrt{\smash[b]{2\met\pt^{\ell}[1-\cos\Delta\phi(\ptvecmiss,\ptvec^{\ell})]}}$ must be less than $160\GeV$.
Additionally, for the single-electron control region, \met\ must be greater than 50\GeV\ to limit the contribution from the \gj\ process, where a jet is misidentified as an electron. %% add a plot
\PH{I do this too, there are a number of sentences where teh explanation is at the end like in German. It makes it hard to read. Instead Additionaly to limit....\met must be greater}


The dielectron (dimuon) control region is defined by exactly two electrons (muons) in addition to the photon, with $60 < m_{\ell\ell} < 120\GeV$, where $m_{\ell\ell}$ is the mass of the dilepton system.
The leading lepton must pass the tight ID requirements, while the trailing lepton only needs to pass the loose ID requirements. 

Finally, in the control regions, the recoil vector $\vec{U} = \ptvecmiss + \sum\limits_\ell\ptvec^{\ell}$ serves as an analogue for the \ptvecmiss\ in the signal region.
In the signal region, the \ptvecmiss\ is a proxy for the vector boson \pt\ while in the control regions, the recoil vector is used instead. 
Thus, the recoil $\vec{U}$ must satisty identical requirements to those for the \ptvecmiss\ in the signal region to keep the control region kinematics as similar as possible to the signal region kinematics.
%% maybe explain more

\subsection{Proxy Samples}
\label{sec:proxy_samples}
\PH{Again this could go into a big table}
To estimate the background due to misidentified electrons, an electron proxy sample is used.
This proxy sample is obtained by identical event selection as that of the signal region but with the pixel-seed veto inverted on the photon candidate object.
Such a photon candidate is referred to as a electron proxy object.
This yields a sample of events with similar kinematics to the signal region and well-identified electron candidates, differing only from the misidentified electron events in that a pixel hit was associated with the photon object.
Thus, these exact events are used to estimate the misidentified electron background after scaling them by the electron-to-photon misidentification rate.

To estimate the background due to misidentified hadrons, a hadron proxy sample is used.
This proxy sample is obtained by identical event selection as that of the signal region but where the photon candidate passes the \egamma\ and \Pgg-specific IDs with except for at least one of the following cuts: $\sieie < 0.01022$ and $\ICH < 0.441\GeV$.
Such a photon candidate is referred to as a hadron proxy object.
This yields a sample of events with similar kinematics to the signal region and well-identified proxies for misidentified hadrons.
Thus, these exact events are used to estimate the misidentified hadron background after scaling them by the hadron-to-photon misidentification rate.

Additional tight and loose hadron proxy objects and samples are made by tightening and loosening the constant term in the \INH\ and \Ig\ requirements on the proxy object.
The specific values for each proxy object are shown in Table~\ref{tab:hadron_proxy}. 

%% double check these values
\begin{table}[htbp]
  \centering
    \begin{tabular}{l | r | r}
      & \INH\ (\GeVns)& \Ig\ (\GeVns) \\
      \hline
      Nominal & 2.792 & 2.176 \\ 
      Loose & 10.910 & 3.630 \\
      Tight & 0.264 & 2.362 
    \end{tabular}
  \caption{Constant terms in the \INH\ and \Ig\ selections for the hadron proxy objects.}
  \label{tab:hadron_proxy}
\end{table}

\subsection{Measurement Samples}
\label{sec:measurement_samples}
\PH{here again, this section is not very clear}

To measure the photon purity and part of the photon efficiency, an EM object+jet measurement sample is formed by requiring an EM object with $\ET > 175\GeV$ and $\abs\eta < 1.44$ plus at least one jet with $\pt > 100\GeV$ and $\abs\eta < 2.5$ which passes the loose jet ID. 
An EM object is a photon candidate that passes the \egamma\ ID with the execption of the following relaxed cuts: $\sieie < 0.015$ and $\ICH < 11.0\GeV$.
Additionally, we apply an $\met < 60\GeV$ cut to make this region orthogonal to the signal region.

To measure the hadron misidentification rate, a hadron proxy+jet measurement sample is formed by replacing the the EM object in the EM object+jet sample with a hadron proxy object, one for each type of hadron proxy.
The hadron proxy+jet samples have the exact same selection as the hadron proxy samples, except that a high-\pt\ jet is required instead of high-\met, minimizing the kinematic differences between the two.

\section{Efficiencies and Scale Factors} 
\label{sec:photoneff}
\PH{I think it might be better to put this before the previous section or at least merge them. The prevous section is out of place. }
While we try to model the CMS detector as accurately as possible with our MC simulations, there are still differences between the behavior of physics objects within the simulations and those from data taken with the detector.
Most importantly, this results in different efficiencies for photons and leptons in data and MC, which we must measure.
To improve our MC, we reweight it by the ratio of the efficiency in data to that in MC, known as the scale factor.

When measuring the scale factor for photons, we factorize the photon ID into the \egamma\ portion and the \Pgg-specific portion. 
The \egamma\ portion of the ID consists of a collection of isolation and shower shape selections designed to reduce the hadron misidentification rate.
We measure the efficiency of the \egamma\ portion  using the ``tag-and-probe'' (TP) method with \Zee\ events as these variables have similar efficiencies for physical electrons and photons. 
The \Pgg-specific portion of the ID consists of the pixel seed veto and non-collision rejection cuts.
We measure the efficiency of \Pgg-specific portion on a sample of physical photons in the EM object+jet measurement sample using a \sieie\ template fit method.

We perform both efficiency estimates as a function of \pt\ with the binning [175,200], [200,250], [250,300], [300,350], [350,400] and [400,$\infty$). 
This binning was chosen based on the number of available events in data for the failing probes fit in the TP method and the background template for the \sieie\ fits, as these samples are the smallest and drive the uncertainty of the methods.

\subsection{\egamma\ ID Efficiency}
\label{sec:idsf}

\input{Analysis/idsf}

\subsection{\Pgg-specific ID Efficiency}
\label{sec:pvsf}

\input{Analysis/pvsf}

\subsection{Lepton Veto Efficiency}
\label{sec:lepton_veto}

The lepton veto requirement in the signal region has a non-unity efficiency over events that do not have genuine leptons, because particles such as pions and protons can mimic leptons to become ``fake leptons'' and cause the event to be rejected. 
To measure the possible difference between data and MC of this lepton veto efficiency, we compare dimuon events in data and MC. 
In a high-purity \Zmm\  sample with the dimuon mass close to $M_{\PZ}$, events with a genuine third lepton is negligibly rare, and therefore the efficiency loss from rejecting events with a third lepton is dominantly due to fake leptons.

For this measurement, collision events are taken from the SingleMuon data set and the MC events from a mixture of Drell-Yan, \ttbar, \PW\PW, \PW\PZ, and \PZ\PZ\ samples. 
We require two muons passing the ``tight'' identification working point defined  with the mass between 61 and 121\GeV. 
The failing category consists of events containing an additional electron or muon object that passes the loose selection criteria while the passing category consists of those without an additional lepton. 
The efficiency is inspected as a function of number of vertices, number of jets, and $H_{\mathrm{T}}$ in the event, and in all cases data and MC are consistent as shown in Figure~\ref{fig:leptonveto_efficiencies}.

\begin{figure}[tbph]
 \centering
  \includegraphics[width=7.5cm,height=7cm]{Analysis/Figures/leptonveto/leptonveto_eff_npv.pdf}
  \includegraphics[width=7.5cm,height=7cm]{Analysis/Figures/leptonveto/leptonveto_eff_njet.pdf}
  \includegraphics[width=7.5cm,height=7cm]{Analysis/Figures/leptonveto/leptonveto_eff_ht.pdf}
  \includegraphics[width=7.5cm,height=7cm]{Analysis/Figures/leptonveto/leptonveto_eff_incl.pdf}
  \caption{
    Lepton veto efficiencies and data/MC scale factors as functions of $N_{\text{vtx}}$, $N_{\text{jet}}$, and $H_{\mathrm{T}}$, and the corresponding inclusive values. 
    While dimuon and \zinvg\ samples have significantly different efficiencies, data and MC agree well within dimuon samples, giving scale factors consisten with 1 almost everywhere. 
    This is true even when additionally requiring a high-\pt\ jet in the event, as seen in the inclusive efficiency plot. 
    Thus, the difference between \zinvg\ and dimuon efficiencies itself is taken as the uncertainty.
  }
 \label{fig:leptonveto_efficiencies}
\end{figure}

It should be noted, however, that the absolute lepton veto efficiency in MC dimuon sample is significantly different from that of the \zinvg\ sample, which more closely features the properties of the signal candidate sample. 
The full difference in the efficiencies between the dimuon and \zinvg\ samples is taken as the systematic uncertainty in the lepton veto scale factor, which is therefore $1.00 \pm 0.02$.

Additionally, a small fraction of events with real leptons pass the lepton veto due to the leptons failing the loose ID requirements. 
This effect is most relevant for \wlng\ events in the signal region and for \zllg\ events in the single lepton control regions. 
We compute a scale factor $\text{SF}_{\text{veto}} = (1. - \epsilon_{\text{data}}) / (1. - \epsilon_{\text{MC}})$ using the data and MC efficiencies for the loose lepton IDs with a flat 1\% uncertainty for the efficiencies.
All scale factors are consistent with unity within the uncertainties. 

This veto scale factor is applied to MC events with a reconstructed lepton that fails the loose ID. 
If there are multiple such leptons in an event, we apply the scale factor only for the hardest muon and electron. 
After applying the scale factors, the final MC yields for \wlng\ in the signal  region and \zllg\ in the single lepton control regions change by less than 0.5\%.

\section{Misidentified electrons}
\label{sec:efake}

\input{Analysis/backgrounds_efake}

\section{Misidentified hadrons}
\label{sec:hfake}

\input{Analysis/backgrounds_hfake}

\section{Irreducible backgrounds}
\label{sec:irreducible}

\input{Analysis/backgrounds_zgwg}

\section{Beam halo}
\label{sec:halo_estimate}

\input{Analysis/backgrounds_halo}

\section{ECAL spikes}
\label{sec:spike_estimate}

\input{Analysis/backgrounds_spike}

\section{Minor SM Backgrounds}
\label{sec:minorsm}

After the full selection described in Section~\ref{sec:event_selection}, the SM \gj, \ttg, $VV\Pgg$, \zllg, and $\PW\to\ell\Pgn$ processes are minor ($\sim$10\%) background processes in the signal region. 
These processes, collectively denoted as minor SM backgrounds, contribute in the signal region when the jet energy is severely mismeasured or the leptons fail to be reconstructed resulting in large \met in the signal region. 
However, the \met is typically aligned with the photon or one of the jets in such cases, and therefore various selections on the kinematic relations between the \met, photons, and jets are used to reduce these backgrounds to a manageable rate. 
The estimates for all five processes are taken from \MGvATNLO simulations at LO in QCD and are listed in Tables~\ref{tab:yield_mask_horizontal} and~\ref{tab:yield_mask_vertical}.

\section{Statistical Interpretation}
\label{sec:interpretation}

The potential signal contribution is extracted from the data via simultaneous fits to the
\ETg\ distributions in the signal and control regions defined in Section~\ref{sec:event_selection}. 
Uncertainties in various quantities are represented by nuisance parameters in the fit. 
Predictions for \zinvg, \wlng, and the beam halo backgrounds are varied in the fit. 
Beam halo is not a major background, but the extraction of its rate requires a fit to the observed distributions in the signal region.

Free parameters of the fit are the yield of \zinvg\ background in each bin of the signal regions (\NZg[i]) and the overall normalization of the beam halo background ($h$). 
Bin-by-bin yields of \wlng\ and \zllg\ samples in all regions are related to the yield of \zinvg\ through the MC prediction through the transfer factors defined in Section~\ref{sec:irreducible}. 
The transfer factors are allowed to shift within the aforementioned theoretical and experimental uncertainties.
The background-only likelihood that is maximized in the fit is
\begin{equation}
  \resizebox{\textwidth}{!}{$
    \begin{aligned}
      \mathcal{L} & = \prod_{i} \Bigg\{ \mathcal{L_{\text{signal}}} \times \mathcal{L_{\text{single-lepton}}} \times \mathcal{L_{\text{dilepton}}} \Bigg\} \times \mathcal{L_{\text{nuisances}}} \\
      & = \prod_{i} \left\{
      \prod_{K=H,V} \mathcal{P}\left( d_{K, i} \left| \TK[,i] (\vec{\theta} \right.) \right) \times \prod_{\ell=\Pe,\Pgm} \mathcal{P}\left( d_{\ell\Pgg, i} \left| \Tl[,i] (\vec{\theta}) \right. \right)
      \times \prod_{\ell=\Pe,\Pgm} \mathcal{P}\left( d_{\ell\ell\Pgg, i} \left| \Tll[,i] (\vec{\theta}) \right. \right)
      \right\}  \times \prod_{j} \mathcal{N}(\theta_j) \\
      & = \prod_{i} \left\{
      \begin{gathered}
        \prod\limits_{K=H,V} \mathcal{P}\biggl( d_{K, i} \biggl| C_{K} \left( \left[1 + \left(\fZW[,i](\vec{\theta})\right)^{-1}\right] \NZg[i] + b_{K, i}(\vec{\theta}) \right) + h \nhalo[,i](\vec{\theta}) \biggr. \biggr) \\
        \times \prod\limits_{\ell=\Pe,\Pgm} \mathcal{P}\biggl( d_{\ell\Pgg, i} \biggl| \frac{\NZg[i]}{\RWl[,i](\vec{\theta}) \fZW[,i] (\vec{\theta})} + b_{\ell\Pgg, i}(\vec{\theta}) \biggr. \biggr) \\
        \times \prod\limits_{\ell=\Pe,\Pgm} \mathcal{P}\biggl( d_{\ell\ell\Pgg, i} \biggl| \frac{\NZg[i]}{\RZll[,i](\vec{\theta})} + b_{\ell\ell\Pgg, i}(\vec{\theta}) \biggr. \biggr)
      \end{gathered} \right\}
      \times \prod_{j} \mathcal{N}(\theta_j),
    \end{aligned}
  $}
\end{equation}
following the notation introduced in Section~\ref{sec:irreducible}, and where $\mathcal{P}(n\vert\lambda)$ is the Poisson probability of $n$ for mean $\lambda$, $\mathcal{N}$ denotes the unit normal distribution, and $d_{K,i}$ is the observed number of events in bin $i$ of region $K$.
Systematic uncertainties on $R_e$, $R_h$, \kqcd, \ksudakov, \kphoton, and the various scale factors $\rho$ are treated as nuisance parameters in the fit and are represented by $\vec{\theta}$. 
Quantities such as \RZll, \RWl, \fZW, \nhalo, and $b_{K}$ appear in the likelihood function as $Q(\vec \theta) = \overline{Q} \sum\limits_{j}\exp(\sigma_{j}\theta_{j})$, where $\overline{Q}$ is the nominal value of the quantity and $\sigma_{j}$ is standard deviation of each systematic uncertainty $\theta_{j}$ on the quantity.

\section{Results}
\label{sec:results}

\input{Analysis/results}
