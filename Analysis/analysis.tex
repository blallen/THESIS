\chapter{The Monophoton Analysis}
\label{chap:analysis}

In this chapter, we discuss the search for dark matter produced in association with a single high-\pt photon.
Our benchmark signal models are the vector and axial dark matter mediators discussed previously. 
However, many Standard Model (SM) processes are also capable of producing events with a single high-\pt\ photon and large \met, whether with real photons, other particles misidentified as photons, or unphysical photon signatures from various  machine and detector processes.

%%% maybe thrown in comparison diagrams here

The most significant are the irreducible backgrounds, where the underlying physics process produces the exact same signature as the signal with only real and properly identified physics objects.
In this case, the irreducible backgrounds are the associated production of a high-energy \Pgg\ with either a \PZ\ boson that subsequently decays to a pair of neutrinos or a \PW\ boson that decays to a charged lepton and a neutrino, with the charged lepton outside of the detector acceptance.
These two processes are denoted as \zinvg\ and \wlng, respectively, and together they account for approximately 70\% of the SM background, with 50\% from the former and 20\% from the latter.
The modeling of the irreducible backgrounds is explained in Section~\ref{sec:irreducible}.
% The total irreducible background rate is estimated using simultaneous fits to the signal and control regions defined in Section~\ref{sec:event_selection} and 

Additional backgrounds arise from events where the candidate photon object is a misidentified electron (Section~\ref{sec:efake}) or an electromagnetic shower caused by hadrons (Section~\ref{sec:hfake}). 
The background events from electron misidentification are mostly \PW\ boson production ($\PW\rightarrow \Pe\Pgn$), whereas those from hadron misidentification are due to multiple sources such as $\PZ(\rightarrow\Pgn\Pgn)+\text{jets}$ and QCD multijets with grossly mismeasured jet energy. 
Misidentification itself is rare, but because these processes have high cross sections, the amount of background is substantial, approximately 15\% and 5\% respectively.
Since object misidentification rates depend on subtle details of the detector, the MC simulation often fails to accurately describe them.
Therefore, the contributions from these background processes are estimated by employing data-driven techniques, where the misidentification rates are measured in data and applied to proxy samples with well-identified electrons or hadrons.

Finally, apparent large energy deposits in ECAL from non-collision processes mimic \gmet\ events and therefore need to be controlled. 
Known sources of such background include bremsstrahlung of beam halo or cosmic ray muons and anomalous ECAL energy deposits resulting from the interaction of particles in the ECAL photodetectors referred to as ``ECAL spikes''. 
These methods used to estimate contributes from these processes are described in detail in Sections~\ref{sec:halo_estimate} and~\ref{sec:spike_estimate}, respectively.

The estimates of the contributions from \zinvg, \wlng, and beam halo processes are allowed to float in the fits to data performed to extract the potential signal contribution and set limits on new physics models. %  described in Section~\ref{sec:interpretation}.
Meanwhile, for all other background processes, the yields in the fits are fixed to the estimates from data-driven methods or MC cross section calculation. 
\section{Dataset}
\label{sec:dataset}

The data sample was collected with a single-photon trigger: \texttt{HLT\_Photon165\_HE10}.
This high-level trigger algorithm is relatively simple, only requiring at least one photon candidate with $\ET > 165\GeV$ reconstructed with a corresponding L1 seed.
The photon candidate must also have $H/E < 0.1$ to discriminate against jets, where $H/E$ is the ratio of HCAL to ECAL energy deposits in the central calorimeter tower corresponding to the candidate.
The photon energy reconstructed at the HLT is less precise relative to that derived later in the offline reconstruction. 
Therefore, the online thresholds in the trigger on both $H/E$ and \ETg are less restrictive than their offline counterparts.

The trigger efficiency is measured to be about 98\% for events passing the analysis selection with $\ETg > 175\GeV$ and the integrated luminosity of the analyzed data sample is $(35.9\pm0.9)$\fbinv~\cite{CMS:2017sdi}.

\subsection{Trigger Efficiency}
\label{sec:triggereff}

\input{Analysis/trigger}

\subsection{Pileup Reweighting}
\label{sec:puweight}

The distribution of the number of pileup interactions inserted into MC events differ from the true pileup distribution, estimated from the measurement of instantaneous luminosity, beam intensity of each proton bunch, and the total cross section of proton inelastic scattering (69.2 $\textrm{mb}^{-1}$).

Figure~\ref{fig:pudist} shows the pileup distributions in data and MC and their ratio. 
Each simulation event has its weight multiplied by the value of the ratio evaluated at the number of true pileup interaction injected into the event.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.48\textwidth]{Calibration/Figures/PUMoriond17.pdf}
  \caption{
    The pileup distributions in data and MC.
  }
  \label{fig:pudist}
\end{figure}

\section{Event Selection}
\label{sec:event_selection}

From the recorded data, events are selected by requiring $\met > 170\GeV$ and at least one photon with $\ETg > 175\GeV$ in the fiducial region of the ECAL barrel ($\abs{\eta} < 1.44$). Events with photons in the endcaps are not considered because the estimate of backgrounds due to beam halo and misidentified hadron are greatly complicated due to the $x$-$y$ grid of the crystals in the endcaps.

Events with a high-\pt\ photon and large \met\ are subjected to further requirements to suppress SM background processes that feature a genuine high-energy photon, but not a significant amount of \met.
One such SM process is \gj, where an apparently large \met\ is often the result of mismeasuring the energy of a jet.
In contrast to signal-like processes, the \met is typically smaller than \ETg\ in these events, so requiring the ratio of \ETg\ to \met to be less than 1.4 rejects this background effectively with little effect on signal efficiency. %% gotta add a plot 
Events are also rejected if the minimum opening angle between \ptvecmiss\ and the directions of the four highest \pt\ jets, \mindphijmet, is less than 0.5. % gotta add a plot 
Only jets with $\pt > 30\GeV$ and $\abs{\eta} < 5$ are considered in the \mindphijmet\ calculation.
In the \gj\ process, rare pathological mismeasurements of \ETg\ also lead to large \met. 
For this reason, the candidate photon \ptvec\ and \ptvecmiss\ must be separated by more than 0.5 radians. %% gotta add a plot

\begin{table}[htbp]
  \centering
  \resizebox{\textwidth}{!}{
    \begin{tabular}{l | l | r}
      Variable & Selection & Motivation \\
      \hline
      $\ETg$ & $ > 175\GeV$ & high-\pt\ photon passing trigger \\
      $\abs{\eta}$ & $ < 1.44$ & region with best background estimates \\
      $\met $ & $ > 170\GeV$ & characteristic signature of dark matter \\
      $\ETg / \met  $ & $ < 1.4$ & reduce jet mismeasurement backgrounds \\
      $\mindphijmet  $ & $ < 0.5$ & reduce jet mismeasurement backgrounds \\
      $\Delta\Phi(\ptvec^\Pgg, \ptvecmiss)  $ & $ > 0.5$ & reduce photon mismeasurement backgrounds \\
    \end{tabular}
    }
  \caption{Baseline selections for all events considered in the analysis.}
  \label{tab:baseline}
\end{table}

The above selections, summarized in Table~\ref{tab:baseline}, constitute the baseline selections common to all regions.
To improve the purity of the signal region, we require a more stringent photon identification as well as additional object vetos.
The contributions from the \zinvg\ and \wlng\ processes to the signal region are modeled by fitting to observed data in control regions where one or two leptons (electrons or muons) are identified in addition to the photon candidate while the contributions from misidentified electrons and hadrons are modeled by proxy regions where some of the selections in the photon identification have been inverted.
The additional requirements for the various signal, control, and proxy regions used in the analysis are described in the following sections.

\subsection{Signal Regions}
\label{sec:signal_regions}

The defining feature of the signal region is the application of both the $\egamma$ and \Pgg-specific portions of the photon ID, given in Tables~\ref{tab:egid} and~\ref{tab:gsid} respectively.
The former reduces the hadron misidentification rate with a collection of isolation and shower shape selections while the latter reduces the electron misidentification rate with the pixel seed veto and rejects non-collisions backgrounds with specifically tailored selections.

In the signal region, events are vetoed if they contain an electron or a muon with $\pt > 10\GeV$ that is separated from the photon by $\dR > 0.5$. This lepton veto rejects SM processes that produce a high-\pt\ photon, \met, and leptons such as \wlng, \ttg, and $VV\Pgg$. %% add a plot ???

Furthermore, to constrain the beam halo normalization, the signal region is split into two parts according to the variable $\phi'$ introduced in Equation~\ref{eqn:phi}. 
The region defined by $\abs{\phi'} < 0.5$ is called the horizontal region, its complement $0.5 < \abs{\phi'} < \pi/2$ is called the vertical region, and the two together are referred to as the combined signal regions.

\subsection{Control Regions}
\label{sec:control_regions}

The single-electron (single-muon) control region is defined by requiring exactly one tight electron (muon) with $\pt > 30\GeV$ and $\abs{\eta} < 2.5\,(2.4)$ in addition to requiring the same photon ID as in the signal regions.
To suppress the contributions from large-\met\ processes other than \wlng, the transverse mass $\mt = \sqrt{\smash[b]{2\met\pt^{\ell}[1-\cos\Delta\phi(\ptvecmiss,\ptvec^{\ell})]}}$ must be less than $160\GeV$.
Additionally, for the single-electron control region, \met\ must be greater than 50\GeV\ to limit the contribution from the \gj\ process, where a jet is misidentified as an electron. %% add a plot

The dielectron (dimuon) control region is defined by exactly two electrons (muons) in addition to the photon, with $60 < m_{\ell\ell} < 120\GeV$, where $m_{\ell\ell}$ is the mass of the dilepton system.
The leading lepton must pass the tight ID requirements, while the trailing lepton only needs to pass the loose ID requirements. 

Finally, in the control regions, the recoil vector $\vec{U} = \ptvecmiss + \sum\limits_\ell\ptvec^{\ell}$ serves as an analogue for the \ptvecmiss\ in the signal region.
In the signal region, the \ptvecmiss\ is a proxy for the vector boson \pt\ while in the control regions, the recoil vector is used instead. 
Thus, the recoil $\vec{U}$ must satisty identical requirements to those for the \ptvecmiss\ in the signal region to keep the control region kinematics as similar as possible to the signal region kinematics.
%% maybe explain more

\subsection{Proxy Samples}
\label{sec:proxy_samples}

To estimate the background due to misidentified electrons, an electron proxy sample is used.
This proxy sample is obtained by identical event selection as that of the signal region but with the pixel-seed veto inverted on the photon candidate object.
Such a photon candidate is referred to as a electron proxy object.
This yields a sample of events with similar kinematics to the signal region and well-identified electron candidates, differing only from the misidentified electron events in that a pixel hit was associated with the photon object.
Thus, these exact events are used to estimate the misidentified electron background after scaling them by the electron-to-photon misidentification rate.

To estimate the background due to misidentified hadrons, a hadron proxy sample is used.
This proxy sample is obtained by identical event selection as that of the signal region but where the photon candidate passes the \egamma\ and \Pgg-specific IDs with except for at least one of the following cuts: $\sieie < 0.01022$ and $\ICH < 0.441\GeV$.
Such a photon candidate is referred to as a hadron proxy object.
This yields a sample of events with similar kinematics to the signal region and well-identified proxies for misidentified hadrons.
Thus, these exact events are used to estimate the misidentified hadron background after scaling them by the hadron-to-photon misidentification rate.

Additional tight and loose hadron proxy objects and samples are made by tightening and loosening the constant term in the \INH\ and \Ig\ requirements on the proxy object.
The specific values for each proxy object are shown in Table~\ref{tab:hadron_proxy}. 

%% double check these values
\begin{table}[htbp]
  \centering
    \begin{tabular}{l | r | r}
      & \INH\ (\GeVns)& \Ig\ (\GeVns) \\
      \hline
      Nominal & 2.792 & 2.176 \\ 
      Loose & 10.910 & 3.630 \\
      Tight & 0.264 & 2.362 
    \end{tabular}
  \caption{Constant terms in the \INH\ and \Ig\ selections for the hadron proxy objects.}
  \label{tab:hadron_proxy}
\end{table}

\subsection{Measurement Samples}
\label{sec:measurement_samples}

To measure the photon purity and part of the photon efficiency, an EM object+jet measurement sample is formed by requiring an EM object with $\ET > 175\GeV$ and $\abs\eta < 1.44$ plus at least one jet with $\pt > 100\GeV$ and $\abs\eta < 2.5$ which passes the loose jet ID. 
An EM object is a photon candidate that passes the \egamma\ ID with the execption of the following relaxed cuts: $\sieie < 0.015$ and $\ICH < 11.0\GeV$.
Additionally, we apply an $\met < 60\GeV$ cut to make this region orthogonal to the signal region.

To measure the hadron misidentification rate, a hadron proxy+jet measurement sample is formed by replacing the the EM object in the EM object+jet sample with a hadron proxy object, one for each type of hadron proxy.
These are exactly the same as the hadron proxy samples, except that a high-\pt\ jet has replaced the high-\met, minizing the kinematic differences between the two.

\section{Photon ID Efficiency Scalefactor}
\label{sec:photoneff}

While we try to model the CMS detector as accurately as possible with our MC simulations, there are still differences between the behavior of photons within the simulations and those from data taken with the detector.
Most importantly, this results in different efficiencies for photons in data dn MC, which we must measure.
To improve our MC, we reweight it by the ratio of the efficiency in data to that in MC, known as the photon efficiency scalefactor.

When measuring the photon efficiency scale factor, we factorize the photon IDinto the \egamma\ portion and the \Pgg-specific portion. 
The \egamma\ portion of the ID consists of a collection of isolation and shower shape selections designed to reduce the hadron misidentification rate.
We measure the efficiency of the \egamma\ portion  using the ``tag-and-probe'' (TP) method with \Zee\ events as these variables have similar efficiencies for physical electrons and photons. 
The \Pgg-specific portion of the ID consists of the pixel seed veto and non-collision rejection cuts.
We measure the efficiency of \Pgg-specific portion on a sample of physical photons in the EM object+jet measurement sample using a \sieie\ template fit method.

We perform both efficiency estimates as a function of \pt\ with the binning [175,200], [200,250], [250,300], [300,350], [350,400] and [400,$\infty$). 
This binning was chosen based on the number of available events in data for the failing probes fit in the TP method and the background template for the \sieie\ fits, as these samples are the smallest and drive the uncertainty of the methods.

\subsection{\egamma\ ID Efficiency}
\label{sec:idsf}

\input{Analysis/idsf}

\subsection{\Pgg-specific ID Efficiency}
\label{sec:pvsf}

\input{Analysis/pvsf}

\section{Misidentified electrons}
\label{sec:efake}

\input{Analysis/backgrounds_efake}

\section{Misidentified hadrons}
\label{sec:hfake}

\input{Analysis/backgrounds_hfake}

\section{Irreducible backgrounds}
\label{sec:irreducible}

\input{Analysis/backgrounds_zgwg}

\section{Beam halo}
\label{sec:halo_estimate}

\input{Analysis/backgrounds_halo}

\section{ECAL spikes}
\label{sec:spike_estimate}

\input{Analysis/backgrounds_spike}

\section{Minor SM Backgrounds}
\label{sec:minorsm}

After the full selection described in Section~\ref{sec:event_selection}, the SM \gj, \ttg, $VV\Pgg$, \zllg, and $\PW\to\ell\Pgn$ processes are minor ($\sim$10\%) background processes in the signal region. 
These processes, collectively denoted as minor SM backgrounds, contribute in the signal region when the jet energy is severely mismeasured or the leptons fail to be reconstructed resulting in large \met in the signal region. 
However, the \met is typically aligned with the photon or one of the jets in such cases, and therefore various selections on the kinematic relations between the \met, photons, and jets are used to reduce these backgrounds to a manageable rate. 
The estimates for all five processes are taken from \MGvATNLO~\cite{} simulations at LO in QCD and are listed in Tables~\ref{tab:yield_mask_horizontal} and~\ref{tab:yield_mask_vertical}.

\section{Statistical Interpretation}
\label{sec:interpretation}

The potential signal contribution is extracted from the data via simultaneous fits to the
\ETg\ distributions in the signal and control regions defined in Section~\ref{sec:event_selection}. 
Uncertainties in various quantities are represented by nuisance parameters in the fit. 
Predictions for \zinvg, \wlng, and the beam halo backgrounds are varied in the fit. 
Beam halo is not a major background, but the extraction of its rate requires a fit to the observed distributions in the signal region.

Free parameters of the fit are the yield of \zinvg\ background in each bin of the signal regions (\NZg[i]) and the overall normalization of the beam halo background ($h$). 
Bin-by-bin yields of \wlng\ and \zllg\ samples in all regions are related to the yield of \zinvg\ through the MC prediction through the transfer factors defined in Section~\ref{sec:irreducible}. 
The transfer factors are allowed to shift within the aforementioned theoretical and experimental uncertainties.
The background-only likelihood that is maximized in the fit is
\begin{equation}
  \resizebox{\textwidth}{!}{$
    \begin{aligned}
      \mathcal{L} & = \prod_{i} \Bigg\{ \mathcal{L_{\text{signal}}} \times \mathcal{L_{\text{single-lepton}}} \times \mathcal{L_{\text{dilepton}}} \Bigg\} \times \mathcal{L_{\text{nuisances}}} \\
      & = \prod_{i} \left\{
      \prod_{K=H,V} \mathcal{P}\left( d_{K, i} \left| \TK[,i] (\vec{\theta} \right.) \right) \times \prod_{\ell=\Pe,\Pgm} \mathcal{P}\left( d_{\ell\Pgg, i} \left| \Tl[,i] (\vec{\theta}) \right. \right)
      \times \prod_{\ell=\Pe,\Pgm} \mathcal{P}\left( d_{\ell\ell\Pgg, i} \left| \Tll[,i] (\vec{\theta}) \right. \right)
      \right\}  \times \prod_{j} \mathcal{N}(\theta_j) \\
      & = \prod_{i} \left\{
      \begin{gathered}
        \prod\limits_{K=H,V} \mathcal{P}\biggl( d_{K, i} \biggl| C_{K} \left( \left[1 + \left(\fZW[,i](\vec{\theta})\right)^{-1}\right] \NZg[i] + b_{K, i}(\vec{\theta}) \right) + h \nhalo[,i](\vec{\theta}) \biggr. \biggr) \\
        \times \prod\limits_{\ell=\Pe,\Pgm} \mathcal{P}\biggl( d_{\ell\Pgg, i} \biggl| \frac{\NZg[i]}{\RWl[,i](\vec{\theta}) \fZW[,i] (\vec{\theta})} + b_{\ell\Pgg, i}(\vec{\theta}) \biggr. \biggr) \\
        \times \prod\limits_{\ell=\Pe,\Pgm} \mathcal{P}\biggl( d_{\ell\ell\Pgg, i} \biggl| \frac{\NZg[i]}{\RZll[,i](\vec{\theta})} + b_{\ell\ell\Pgg, i}(\vec{\theta}) \biggr. \biggr)
      \end{gathered} \right\}
      \times \prod_{j} \mathcal{N}(\theta_j),
    \end{aligned}
  $}
\end{equation}
following the notation introduced in Section~\ref{sec:irreducible}, and where $\mathcal{P}(n\vert\lambda)$ is the Poisson probability of $n$ for mean $\lambda$, $\mathcal{N}$ denotes the unit normal distribution, and $d_{K,i}$ is the observed number of events in bin $i$ of region $K$.
Systematic uncertainties on $R_e$, $R_h$, \kqcd, \ksudakov, \kphoton, and the various scale factors $\rho$ are treated as nuisance parameters in the fit and are represented by $\vec{\theta}$. 
Quantities such as \RZll, \RWl, \fZW, \nhalo, and $b_{K}$ appear in the likelihood function as $Q(\vec \theta) = \overline{Q} \sum\limits_{j}\exp(\sigma_{j}\theta_{j})$, where $\overline{Q}$ is the nominal value of the quantity and $\sigma_{j}$ is standard deviation of each systematic uncertainty $\theta_{j}$ on the quantity.

\section{Results}
\label{sec:results}

\input{Analysis/results}
