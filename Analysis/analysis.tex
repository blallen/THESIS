\chapter{The Monophoton Analysis}
\label{chap:analysis}

In this chapter, we discuss the search for dark matter produced in association with a single high-\pt photon.
Our benchmark signal models are the vector and axial dark matter mediators discussed previously. 
However, many Standard Model (SM) processes are also capable of producing events with a single high-\pt\ photon and large \met, whether with real photons, other particles misidentified as photons, or unphysical photon signatures from various  machine and detector processes. \PH{However, many doesn't read well. I think this first paragraph deserves an actual introduction to the analysis and not an immediate dive into the details}
\PH{You immediately start talkinga bout the backgrounds. Would be better to put some diagrams and discussion about the final state. This you can tie in with the above comment.}

%%% maybe thrown in comparison diagrams here

The most significant are the irreducible backgrounds, where the underlying physics process produces the exact same signature as the signal with only real and properly identified physics objects.
In this case, the irreducible backgrounds are the associated production of a high-energy \Pgg\ with either a \PZ\ boson that subsequently decays to a pair of neutrinos or a \PW\ boson that decays to a charged lepton and a neutrino, with the charged lepton outside of the detector acceptance.
These two processes are denoted as \zinvg\ and \wlng, respectively, and together they account for approximately 70\% of the SM background, with 50\% from the former and 20\% from the latter.
The modeling of the irreducible backgrounds is explained in Section~\ref{sec:irreducible}.
% The total irreducible background rate is estimated using simultaneous fits to the signal and control regions defined in Section~\ref{sec:event_selection} and 

Additional backgrounds arise from events where the candidate photon object is a misidentified electron (Section~\ref{sec:efake}) or an electromagnetic shower caused by hadrons (Section~\ref{sec:hfake}). 
The background events from electron misidentification are mostly \PW\ boson production ($\PW\rightarrow \Pe\Pgn$), whereas those from hadron misidentification are due to multiple sources such as $\PZ(\rightarrow\Pgn\Pgn)+\text{jets}$ and QCD multijets with grossly mismeasured jet energy. 
Misidentification itself is rare, but because these processes have high cross sections, the amount of background is substantial, approximately 15\% and 5\% respectively.
Since object misidentification rates depend on subtle details of the detector, the MC simulation often fails to accurately describe them.
Therefore, the contributions from these background processes are estimated by employing data-driven techniques, where the misidentification rates are measured in data and applied to proxy samples with well-identified electrons or hadrons.

Finally, apparent large energy deposits in ECAL from non-collision processes mimic \gmet\ events and therefore need to be controlled. 
Known sources of such background include bremsstrahlung of beam halo or cosmic ray muons and anomalous ECAL energy deposits resulting from the interaction of particles in the ECAL photodetectors referred to as ``ECAL spikes''. 
These methods used to estimate the contributions from these processes are described in detail in Sections~\ref{sec:halo_estimate} and~\ref{sec:spike_estimate}, respectively.

The estimates of the contributions from \zinvg, \wlng, and beam halo processes are allowed to float in the fits to data performed to extract the potential signal contribution and set limits on new physics models. %  described in Section~\ref{sec:interpretation}.
Meanwhile, for all other background processes, the yields in the fits are fixed to the estimates from data-driven methods or MC cross section calculation. 
\PH{The above paragraph doesn't fit. You bring up fit out of nowhere and floating is jargon. I would just remove this and outline this when you do the signal extraction}


\section{Dataset}
\label{sec:dataset}

The data sample was collected with a single-photon trigger: \texttt{HLT\_Photon165\_HE10}.
This high-level trigger algorithm is relatively simple, only requiring at least one photon candidate with $\ET > 165\GeV$ reconstructed with a corresponding L1 seed.
The photon candidate must also have $H/E < 0.1$ to discriminate against jets, where $H/E$ is the ratio of HCAL to ECAL energy deposits in the central calorimeter tower corresponding to the candidate.
The photon energy reconstructed at the HLT is less precise relative to that derived later in the offline reconstruction. 
Therefore, the online thresholds in the trigger on both $H/E$ and \ETg are less restrictive than their offline counterparts.

The trigger efficiency is measured to be about 98\% for events passing the analysis selection with $\ETg > 175\GeV$ and the integrated luminosity of the analyzed data sample is $(35.9\pm0.9)$\fbinv~\cite{CMS:2017sdi}.

\subsection{Trigger Efficiency}
\label{sec:triggereff}

\input{Analysis/trigger}

\subsection{Pileup Reweighting}
\label{sec:puweight}

The distribution of the number of pileup interactions inserted into MC events differ from the true pileup distribution, estimated from the measurement of instantaneous luminosity, beam intensity of each proton bunch, and the total cross section of proton inelastic scattering (69.2 $\textrm{mb}^{-1}$). \PH{we should know htis calculation each proton bunch multiplied by the total cross...}

Figure~\ref{fig:pudist} shows the pileup distributions in data and MC and the ratio of the two. 
Each simulated MC event has its weight multiplied by the value of the ratio evaluated at the number of true pileup interaction injected into the event in order to reproduce the observed pileup distribution in the MC samples. 

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.48\textwidth]{Analysis/Figures/PUMoriond17.pdf}
  \caption{
    The pileup distributions in data and MC.
    The ratio of the two is used to reweight the MC samples.
  }
  \label{fig:pudist}
\end{figure}

\section{Event Selection}
\label{sec:event_selection}

From the recorded data, events are selected by requiring at least one photon with $\ETg > 175\GeV$ in the fiducial region of the ECAL barrel ($\abs{\eta} < 1.44$).
Events with photons in the endcaps are not considered because the estimation of backgrounds from beam halo and misidentified hadrons is greatly complicated because the crystals in the ECAL endcaps do not form a grid in $\eta$ and $\phi$.

To reduce the hadron misidentification rate, we require the leading photon to pass the the \egamma\ ID, a collection of isolation and shower shape selections given in Table~\ref{tab:egid}.
Furthermore, to reduce the electron misidentification rate and reject non-collision backgrounds, we require the leading photon to pass the \Pgg-specific ID, which consists of the pixel seed veto and additional selections given Table~\ref{tab:gsid}.

\begin{figure}[htbp]
  \centering
  \resizebox{\textwidth}{!}{
    \includegraphics[]{Analysis/Figures/selection/recoilScan.pdf}
    \includegraphics[]{Analysis/Figures/selection/phoPtOverMet.pdf}
  }
  \caption{
    Left: The \met\ distribution in the signal region before applying any selection on \met. 
    The \gj\ background dominates below 150\GeV, while the DM signal and irreducible backgrounds dominate in the tails.
    Right: The ratio of \ETg\ to \met\ after requiring $\met > 170\GeV$.
    Signal-like events are clustered near 1 while \gj\ events dominate at values above 1.4.  
  }
  \label{fig:metcuts}
\end{figure}

Events with a high-\pt\ photon are subjected to further requirements to suppress SM background processes that feature a genuine high-energy photon, but not a significant amount of \met.
As shown on the left of Figure~\ref{fig:metcuts}, the main SM process is \gj, mismeasuring the energy of a jet results in a sizeable amount of artificial \met.
To reject the vast majority of the \gj\ background, we require $\met > 170\GeV$.
Looking at the right of Figure~\ref{fig:metcuts}, we see that the \met is typically smaller than \ETg for the remaining \gj\ events while the \met\ and \ETg\ are approximately equal for signal-like processes.
Thus, we require the ratio of \ETg\ to \met to be less than 1.4 to reject this remainder of this background effectively with little effect on signal efficiency.

\begin{figure}[htbp]
  \centering
  \resizebox{\textwidth}{!}{
    \includegraphics[]{Analysis/Figures/selection/dPhiJetMetMin.pdf}
    \includegraphics[]{Analysis/Figures/selection/dPhiPhoMet.pdf}
  }
  \caption{
    Signal region N-1 plots for the \mindphijmet\ (left), and \dphigmet\ (right) selections.
    The requirement that the angular separations exceed 0.5 radians was chosen to reduce the rate of events with artificial \met\ from energy mismeasurement of jets and photons, respectively.
  }
  \label{fig:n-1}
\end{figure}

To protect against artificial \met\ from jet mismeasurement, events are rejected if the minimum opening angle between \ptvecmiss\ and the directions of the four highest \pt\ jets, \mindphijmet, is less than 0.5; only jets with $\pt > 30\GeV$ and $\abs{\eta} < 5$ are considered in the \mindphijmet\ calculation. 
Rare pathological mismeasurements of \ETg\ also lead to large \met. 
For this reason, the candidate photon \ptvec\ and \ptvecmiss\ must be separated by more than 0.5 radians.
Figure~\ref{fig:n-1} shows the N-1 plots in the signal region for these two angular selections.

Additionally, events are vetoed if they contain an electron or a muon with $\pt > 10\GeV$ that is separated from the photon by $\dR > 0.5$. This lepton veto rejects SM processes that produce a high-\pt\ photon, \met, and leptons such as \wlng, \ttg, and $VV\Pgg$.

\begin{table}[htbp]
  \centering
    \begin{tabular}{l | l | r}
      Variable & Selection & Motivation \\
      \hline
      $\ETg$ & $ > 175\GeV$ & high-\pt\ photon passing trigger \\
      $\abs{\eta}$ & $ < 1.44$ & region with best background estimates \\
      \egamma\ ID & Pass & reject hadronic background \\
      \Pgg-specific ID & Pass & reject electron and non-collision backgrounds \\ 
      $\met $ & $ > 170\GeV$ & characteristic signature of dark matter \\
      $\ETg / \met  $ & $ < 1.4$ & reduce jet mismeasurement backgrounds \\
      $\mindphijmet  $ & $ > 0.5$ & reduce jet mismeasurement backgrounds \\
      $\dphigmet  $ & $ > 0.5$ & reduce photon mismeasurement backgrounds \\
      Lepton Veto & Pass & reject backgrounds with real leptons \\
    \end{tabular}
  \caption{Selections for the signal region.}
  \label{tab:signal_region}
\end{table}

The above selections, summarized in Table~\ref{tab:signal_region}, constitute the signal selection for the analysis. 

\section{Efficiencies and Scale Factors} 
\label{sec:photoneff}
While we try to model the CMS detector as accurately as possible with our MC simulations, there are still differences between the behavior of physics objects within the simulations and those from data taken with the detector.
Most importantly, this results in different efficiencies for photons and leptons in data and MC, which we must measure.
To improve our MC, we reweight it by the ratio of the efficiency in data to that in MC, known as the scale factor.

When measuring the scale factor for photons, we factorize the photon ID into the \egamma\ portion and the \Pgg-specific portion. 
The \egamma\ portion of the ID consists of a collection of isolation and shower shape selections designed to reduce the hadron misidentification rate.
We measure the efficiency of the \egamma\ portion  using the ``tag-and-probe'' (TP) method with \Zee\ events as these variables have similar efficiencies for physical electrons and photons. 
The \Pgg-specific portion of the ID consists of the pixel seed veto and non-collision rejection cuts.
We measure the efficiency of \Pgg-specific portion on a sample of physical photons in the EM object+jet measurement sample using a \sieie\ template fit method.

We perform both efficiency estimates as a function of \pt\ with the binning [175,200], [200,250], [250,300], [300,350], [350,400] and [400,$\infty$). 
This binning was chosen based on the number of available events in data for the failing probes fit in the TP method and the background template for the \sieie\ fits, as these samples are the smallest and drive the uncertainty of the methods.

\subsection{\egamma\ ID Efficiency}
\label{sec:idsf}

\input{Analysis/idsf}

\subsection{\Pgg-specific ID Efficiency}
\label{sec:pvsf}

\input{Analysis/pvsf}

\subsection{Lepton Veto Efficiency}
\label{sec:lepton_veto}

The lepton veto requirement in the signal region has a non-unity efficiency over events that do not have genuine leptons, because particles such as pions and protons can mimic leptons to become ``fake leptons'' and cause the event to be rejected. 
To measure the possible difference between data and MC of this lepton veto efficiency, we compare dimuon events in data and MC. 
In a high-purity \Zmm\  sample with the dimuon mass close to $M_{\PZ}$, events with a genuine third lepton is negligibly rare, and therefore the efficiency loss from rejecting events with a third lepton is dominantly due to fake leptons.

For this measurement, collision events are taken from the SingleMuon data set and the MC events from a mixture of Drell-Yan, \ttbar, \PW\PW, \PW\PZ, and \PZ\PZ\ samples. 
We require two muons passing the ``tight'' identification working point defined  with the mass between 61 and 121\GeV. 
The failing category consists of events containing an additional electron or muon object that passes the loose selection criteria while the passing category consists of those without an additional lepton. 
The efficiency is inspected as a function of number of vertices, number of jets, and $H_{\mathrm{T}}$ in the event, and in all cases data and MC are consistent as shown in Figure~\ref{fig:leptonveto_efficiencies}.

\begin{figure}[tbph]
 \centering
  \includegraphics[width=7.5cm,height=7cm]{Analysis/Figures/leptonveto/leptonveto_eff_npv.pdf}
  \includegraphics[width=7.5cm,height=7cm]{Analysis/Figures/leptonveto/leptonveto_eff_njet.pdf}
  \includegraphics[width=7.5cm,height=7cm]{Analysis/Figures/leptonveto/leptonveto_eff_ht.pdf}
  \includegraphics[width=7.5cm,height=7cm]{Analysis/Figures/leptonveto/leptonveto_eff_incl.pdf}
  \caption{
    Lepton veto efficiencies and data/MC scale factors as functions of $N_{\text{vtx}}$, $N_{\text{jet}}$, and $H_{\mathrm{T}}$, and the corresponding inclusive values. 
    While dimuon and \zinvg\ samples have significantly different efficiencies, data and MC agree well within dimuon samples, giving scale factors consisten with 1 almost everywhere. 
    This is true even when additionally requiring a high-\pt\ jet in the event, as seen in the inclusive efficiency plot. 
    Thus, the difference between \zinvg\ and dimuon efficiencies itself is taken as the uncertainty.
  }
 \label{fig:leptonveto_efficiencies}
\end{figure}

It should be noted, however, that the absolute lepton veto efficiency in MC dimuon sample is significantly different from that of the \zinvg\ sample, which more closely features the properties of the signal candidate sample. 
The full difference in the efficiencies between the dimuon and \zinvg\ samples is taken as the systematic uncertainty in the lepton veto scale factor, which is therefore $1.00 \pm 0.02$.

Additionally, a small fraction of events with real leptons pass the lepton veto due to the leptons failing the loose ID requirements. 
This effect is most relevant for \wlng\ events in the signal region and for \zllg\ events in the single lepton control regions. 
We compute a scale factor $\text{SF}_{\text{veto}} = (1. - \epsilon_{\text{data}}) / (1. - \epsilon_{\text{MC}})$ using the data and MC efficiencies for the loose lepton IDs with a flat 1\% uncertainty for the efficiencies.
All scale factors are consistent with unity within the uncertainties. 

This veto scale factor is applied to MC events with a reconstructed lepton that fails the loose ID. 
If there are multiple such leptons in an event, we apply the scale factor only for the hardest muon and electron. 
After applying the scale factors, the final MC yields for \wlng\ in the signal  region and \zllg\ in the single lepton control regions change by less than 0.5\%.

\section{Misidentified electrons}
\label{sec:efake}

\input{Analysis/backgrounds_efake}

\section{Misidentified hadrons}
\label{sec:hfake}

\input{Analysis/backgrounds_hfake}

\section{Irreducible backgrounds}
\label{sec:irreducible}

\input{Analysis/backgrounds_zgwg}

\section{Beam halo}
\label{sec:halo_estimate}

\input{Analysis/backgrounds_halo}

\section{ECAL spikes}
\label{sec:spike_estimate}

\input{Analysis/backgrounds_spike}

\section{Minor SM Backgrounds}
\label{sec:minorsm}

After the full selection described in Section~\ref{sec:event_selection}, the SM \gj, \ttg, $VV\Pgg$, \zllg, and $\PW\to\ell\Pgn$ processes are minor ($\sim$10\%) background processes in the signal region. 
These processes, collectively denoted as minor SM backgrounds, contribute in the signal region when the jet energy is severely mismeasured or the leptons fail to be reconstructed resulting in large \met in the signal region. 
However, the \met is typically aligned with the photon or one of the jets in such cases, and therefore various selections on the kinematic relations between the \met, photons, and jets are used to reduce these backgrounds to a manageable rate. 
The estimates for all five processes are taken from \MGvATNLO simulations at LO in QCD and are listed in Tables~\ref{tab:yield_mask_horizontal} and~\ref{tab:yield_mask_vertical}.

\section{Statistical Interpretation}
\label{sec:interpretation}

The potential signal contribution is extracted from the data via simultaneous fits to the
\ETg\ distributions in the signal and control regions defined in Section~\ref{sec:event_selection}. 
Uncertainties in various quantities are represented by nuisance parameters in the fit. 
Predictions for \zinvg, \wlng, and the beam halo backgrounds are varied in the fit. 
Beam halo is not a major background, but the extraction of its rate requires a fit to the observed distributions in the signal region.

Free parameters of the fit are the yield of \zinvg\ background in each bin of the signal regions (\NZg[i]) and the overall normalization of the beam halo background ($h$). 
Bin-by-bin yields of \wlng\ and \zllg\ samples in all regions are related to the yield of \zinvg\ through the MC prediction through the transfer factors defined in Section~\ref{sec:irreducible}. 
The transfer factors are allowed to shift within the aforementioned theoretical and experimental uncertainties.
The background-only likelihood that is maximized in the fit is
\begin{equation}
  \resizebox{\textwidth}{!}{$
    \begin{aligned}
      \mathcal{L} & = \prod_{i} \Bigg\{ \mathcal{L_{\text{signal}}} \times \mathcal{L_{\text{single-lepton}}} \times \mathcal{L_{\text{dilepton}}} \Bigg\} \times \mathcal{L_{\text{nuisances}}} \\
      & = \prod_{i} \left\{
      \prod_{K=H,V} \mathcal{P}\left( d_{K, i} \left| \TK[,i] (\vec{\theta} \right.) \right) \times \prod_{\ell=\Pe,\Pgm} \mathcal{P}\left( d_{\ell\Pgg, i} \left| \Tl[,i] (\vec{\theta}) \right. \right)
      \times \prod_{\ell=\Pe,\Pgm} \mathcal{P}\left( d_{\ell\ell\Pgg, i} \left| \Tll[,i] (\vec{\theta}) \right. \right)
      \right\}  \times \prod_{j} \mathcal{N}(\theta_j) \\
      & = \prod_{i} \left\{
      \begin{gathered}
        \prod\limits_{K=H,V} \mathcal{P}\biggl( d_{K, i} \biggl| C_{K} \left( \left[1 + \left(\fZW[,i](\vec{\theta})\right)^{-1}\right] \NZg[i] + b_{K, i}(\vec{\theta}) \right) + h \nhalo[,i](\vec{\theta}) \biggr. \biggr) \\
        \times \prod\limits_{\ell=\Pe,\Pgm} \mathcal{P}\biggl( d_{\ell\Pgg, i} \biggl| \frac{\NZg[i]}{\RWl[,i](\vec{\theta}) \fZW[,i] (\vec{\theta})} + b_{\ell\Pgg, i}(\vec{\theta}) \biggr. \biggr) \\
        \times \prod\limits_{\ell=\Pe,\Pgm} \mathcal{P}\biggl( d_{\ell\ell\Pgg, i} \biggl| \frac{\NZg[i]}{\RZll[,i](\vec{\theta})} + b_{\ell\ell\Pgg, i}(\vec{\theta}) \biggr. \biggr)
      \end{gathered} \right\}
      \times \prod_{j} \mathcal{N}(\theta_j),
    \end{aligned}
  $}
\end{equation}
following the notation introduced in Section~\ref{sec:irreducible}, and where $\mathcal{P}(n\vert\lambda)$ is the Poisson probability of $n$ for mean $\lambda$, $\mathcal{N}$ denotes the unit normal distribution, and $d_{K,i}$ is the observed number of events in bin $i$ of region $K$.
Systematic uncertainties on $R_e$, $R_h$, \kqcd, \ksudakov, \kphoton, and the various scale factors $\rho$ are treated as nuisance parameters in the fit and are represented by $\vec{\theta}$. 
Quantities such as \RZll, \RWl, \fZW, \nhalo, and $b_{K}$ appear in the likelihood function as $Q(\vec \theta) = \overline{Q} \sum\limits_{j}\exp(\sigma_{j}\theta_{j})$, where $\overline{Q}$ is the nominal value of the quantity and $\sigma_{j}$ is standard deviation of each systematic uncertainty $\theta_{j}$ on the quantity.

\section{Results}
\label{sec:results}

\input{Analysis/results}
